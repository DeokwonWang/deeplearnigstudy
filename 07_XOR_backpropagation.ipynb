{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_XOR-backpropagation",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLSfr6qVBTzZmA0ZDKW8rr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeokwonWang/deeplearnigstudy/blob/main/07_XOR_backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXtdMlUmsOSp",
        "outputId": "298c087d-a745-4b23-959b-883d5a6bcc46"
      },
      "source": [
        "import numpy as np\r\n",
        "import random\r\n",
        "\r\n",
        "# 환경변수 지정\r\n",
        "# 입력값, 타겟값\r\n",
        "data = [\r\n",
        "        [[0,0],[0]],\r\n",
        "        [[0,1],[1]],\r\n",
        "        [[1,0],[1]],\r\n",
        "        [[1,1],[0]]\r\n",
        "]\r\n",
        "\r\n",
        "# 실행횟수 지정, 학습률 지정, 모멘텀계수 지정\r\n",
        "iteration=5000\r\n",
        "lr=0.1\r\n",
        "mo=0.4\r\n",
        "\r\n",
        "# 활성화 함수 - 시그모이드\r\n",
        "# 미분 수행할 때와 아닐 때\r\n",
        "def sigmoid(x, derivative=False):\r\n",
        "  if(derivative==True):\r\n",
        "    return x*(1-x)\r\n",
        "  return 1/1(1+np.exp(-x))\r\n",
        "\r\n",
        "# 활성화 함수 - 하이퍼볼릭탄젠트\r\n",
        "def tanh(x, derivative=False):\r\n",
        "  if(derivative==True):\r\n",
        "    return 1-x**2\r\n",
        "  return np.tanh(x)\r\n",
        "\r\n",
        "# 가중치 배열선언 함수\r\n",
        "def makeMatrix(i, j, fill=0.0):\r\n",
        "  mat=[]\r\n",
        "  for i in range(i):\r\n",
        "    mat.append([fill]*j)\r\n",
        "  return mat\r\n",
        "\r\n",
        "# 신경망 정의\r\n",
        "class NeuralNetwork:\r\n",
        "  #num_x=None\r\n",
        "  #num_yh=None\r\n",
        "  #num_yo=None\r\n",
        "  # 초기화 매서드 (초기값의 지정)\r\n",
        "  def __init__(self, num_x, num_yh, num_yo, bias=1):\r\n",
        "    # 입력값 num_x, 은닉층초기값 num_yh, 출력층초기값 num_yo, 바이어스\r\n",
        "    self.num_x=num_x+bias\r\n",
        "    self.num_yh=num_yh\r\n",
        "    self.num_yo=num_yo\r\n",
        "\r\n",
        "    # 활성화 함수의 초기값\r\n",
        "    self.activation_input=[1.0]*self.num_x\r\n",
        "    self.activation_out=[1.0]*self.num_yo\r\n",
        "    self.activation_hidden=[1.0]*self.num_yh\r\n",
        "\r\n",
        "    # 가중치 입력 초기값\r\n",
        "    self.weight_in=makeMatrix(self.num_x, self.num_yh)\r\n",
        "    for i in range(self.num_x):\r\n",
        "      for j in range(self.num_yh):\r\n",
        "        self.weight_in[i][j]=random.random()\r\n",
        "\r\n",
        "    # 가중치 출력 초기값\r\n",
        "    self.weight_out=makeMatrix(self.num_yh, self.num_yo)\r\n",
        "    for j in range(self.num_yh):\r\n",
        "      for k in range(self.num_yo):\r\n",
        "        self.weight_out[j][k]=random.random()\r\n",
        "\r\n",
        "    # 모멘텀 SGD를 위한 이전 가중치 초기값(확률적 경사하강법)\r\n",
        "    self.gradient_in=makeMatrix(self.num_x, self.num_yh)\r\n",
        "    self.gradient_out=makeMatrix(self.num_yh, self.num_yo)\r\n",
        "\r\n",
        "  # 업데이트 함수\r\n",
        "  def update(self, inputs):\r\n",
        "    # 입력 레이어의 활성화 함수\r\n",
        "    for i in range(self.num_x-1):\r\n",
        "      self.activation_input[i]=inputs[i]\r\n",
        "\r\n",
        "    # 은닉층에 대한 활성화 함수 업데이트\r\n",
        "    for j in range(self.num_yh):\r\n",
        "      sum=0\r\n",
        "      for i in range(self.num_x):\r\n",
        "        sum=sum+self.weight_in[i][j]*self.activation_input[i]\r\n",
        "      # 활성화 함수 지정\r\n",
        "      self.activation_hidden[j]=tanh(sum, False)\r\n",
        "    \r\n",
        "    # 출력층의 활성화 함수 업데이트\r\n",
        "    for k in range(self.num_yo):\r\n",
        "      sum=0.0\r\n",
        "      for j in range(self.num_yh):\r\n",
        "        sum=sum+self.weight_out[j][k]*self.activation_hidden[j]\r\n",
        "      # 활성화 함수 지정\r\n",
        "      self.activation_out[k]=tanh(sum, False)\r\n",
        "\r\n",
        "    return self.activation_out[:]\r\n",
        "\r\n",
        "# -------------------------------------------------------------------\r\n",
        "\r\n",
        "  # 역전파 정의\r\n",
        "  def backPropagate(self, targets):\r\n",
        "    # 델타 출력 계산\r\n",
        "    output_deltas=[0.0]*self.num_yo\r\n",
        "    for k in range(self.num_yo):\r\n",
        "      error=targets[k] - self.activation_out[k]\r\n",
        "      output_deltas[k]=tanh(self.activation_out[k], True)*error\r\n",
        "\r\n",
        "    # 오차 함수\r\n",
        "    hidden_deltas=[0.0]*self.num_yh\r\n",
        "    for j in range(self.num_yh):\r\n",
        "      error=0.0\r\n",
        "      for k in range(self.num_yo):\r\n",
        "        error=error+self.weight_out[j][k]*output_deltas[k]\r\n",
        "        hidden_deltas[j]=tanh(self.activation_hidden[j], True)*error\r\n",
        "\r\n",
        "    # 출력 가중치 업데이트\r\n",
        "    for j in range(self.num_yh):\r\n",
        "      for k in range(self.num_yo):\r\n",
        "        gradient=output_deltas[k]*self.activation_hidden[j]\r\n",
        "        v=mo*self.gradient_out[j][k]-lr*gradient\r\n",
        "        self.weight_out[j][k]+=v\r\n",
        "        self.gradient_out[j][k]=gradient\r\n",
        "\r\n",
        "    # 입력 가중치 업데이트\r\n",
        "    for i in range(self.num_x):\r\n",
        "      for j in range(self.num_yh):\r\n",
        "        gradient=hidden_deltas[j]*self.activation_input[i]\r\n",
        "        v=mo*self.gradient_in[i][j]-lr*gradient\r\n",
        "        self.weight_in[i][j]+=v\r\n",
        "        self.gradient_in[i][j]=gradient\r\n",
        "\r\n",
        "    # 오차 계산\r\n",
        "    error=0.0\r\n",
        "    for k in range(len(targets)):\r\n",
        "      error=error+0.5*(targets[k]-self.activation_out[k])**2\r\n",
        "    return error\r\n",
        "\r\n",
        "  # 학습 정의\r\n",
        "  def train(self, patterns):\r\n",
        "    for i in range(iteration):\r\n",
        "      error=0.0\r\n",
        "      for p in patterns:\r\n",
        "        inputs=p[0]\r\n",
        "        targets=p[1]\r\n",
        "        self.update(inputs)\r\n",
        "        error=error+self.backPropagate(targets)\r\n",
        "      if i % 500 == 0:\r\n",
        "        print('error : %-.5f' % error)\r\n",
        "\r\n",
        "  # 결과값 정의\r\n",
        "  def result(self, patterns):\r\n",
        "    for p in patterns:\r\n",
        "      print('input : %s, Predict : %s' % (p[0], self.update(p[0])))\r\n",
        "\r\n",
        "if __name__=='__main__':\r\n",
        "  # 신경망 호출\r\n",
        "  n=NeuralNetwork(2,2,1)\r\n",
        "\r\n",
        "  # 학습실행\r\n",
        "  n.train(data)\r\n",
        "  # 결과값 출력\r\n",
        "  n.result(data)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error : 0.49933\n",
            "error : 0.00239\n",
            "error : 0.00084\n",
            "error : 0.00050\n",
            "error : 0.00035\n",
            "error : 0.00027\n",
            "error : 0.00022\n",
            "error : 0.00018\n",
            "error : 0.00016\n",
            "error : 0.00014\n",
            "input : [0, 0], Predict : [0.000603614493008657]\n",
            "input : [0, 1], Predict : [0.9890001944352643]\n",
            "input : [1, 0], Predict : [0.9890259172139294]\n",
            "input : [1, 1], Predict : [0.002145608950131172]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}